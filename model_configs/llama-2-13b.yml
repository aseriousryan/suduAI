model_type: llama-cpp
model_path: models/llama-2-13b-chat.Q5_K_M.gguf
context_length: 4096
max_tokens: 2048
temperature: 0.0
n_gpu_layers: 40
stop: ['</s>']
last_n_tokens: 2048
prompt_template: "[INST] <<SYS>>\n{system_message}\n<</SYS>>[/INST]\n\n[INST]{prompt}[/INST]"

prefix_template: "[INST] <<SYS>>\n{prefix_text}\n<</SYS>>[/INST]\n"

suffix_template: "[INST] {suffix_text} [/INST]"