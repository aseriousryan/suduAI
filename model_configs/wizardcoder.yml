model_type: llama-cpp
model_path: models/wizardcoder-python-7b-v1.0.Q8_0.gguf
context_length: 4096
max_tokens: 4096
temperature: 0.0
n_gpu_layers: 30
stop: ['</s>']
last_n_tokens: 2048
prompt_template: "{system_message}\n\n### Instruction:\n{prompt}\n\n###Response:"
prefix_template: "{prefix_text}\n\n"
suffix_template: "### Instruction:\n{suffix_text}\n\n### Response:"