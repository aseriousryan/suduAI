model_type: llama-cpp
model_path: models/yi-34b-chat.Q4_0.gguf
context_length: 2048
max_tokens: 2048
temperature: 0.0
n_gpu_layers: 30
stop: []
last_n_tokens: 2048
prompt_template: "<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
"

prefix_template: "<|im_start|>system

{prefix_text} <|im_end|>

"

suffix_template: "<|im_start|>user

{suffix_text} <|im_end|>

<|im_start|>assistant

"