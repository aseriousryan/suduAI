model_type: llama-cpp
model_path: models/llama-2-7b-chat.Q8_0.gguf
context_length: 4096
max_tokens: 2048
temperature: 0.0
n_gpu_layers: 50
stop: ['</s>']
last_n_tokens: 2048
prompt_template: "[INST] <<SYS>>\n{system_message}\n<</SYS>>[/INST]\n\n[INST]{prompt}[/INST]"

prefix_template: "[INST] <<SYS>>\n{prefix_text}\n<</SYS>>[/INST]\n"

suffix_template: "[INST] {suffix_text} [/INST]"