model_type: llama-cpp
model_path: models/leoscorpius-7b.Q5_K_M.gguf
context_length: 32768
max_tokens: 4096
temperature: 0.0
stop: []
last_n_tokens: 2048
n_gpu_layers: 35
n_batch: 512
prompt_template: "### Instruction:
{prompt}

### Response:\n
"

prefix_template: "### Instruction: {prefix_text}\n\n"

suffix_template: "{suffix_text}\n### Response:"
