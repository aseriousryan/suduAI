{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Variation Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seriousco/anaconda3/envs/jiaxin-env/lib/python3.10/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! offload_kqv is not default parameter.\n",
      "                offload_kqv was transferred to model_kwargs.\n",
      "                Please confirm that offload_kqv is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from models/openchat-3.5-0106.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = openchat_openchat-3.5-0106\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.11 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.63 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```\n",
      "What is the total amount due for cash transactions from last month?\n",
      "Please provide the outstanding balance for cash sales during the previous month.\n",
      "Can you give me the remaining cash payment balance for the last month's sales?\n",
      "Could you tell me the current outstanding cash sales balance from the past month?\n",
      "Kindly inform me of the total cash sales balance that is still pending from last month.\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1329.74 ms\n",
      "llama_print_timings:      sample time =      25.25 ms /    86 runs   (    0.29 ms per token,  3405.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29445.06 ms /   170 tokens (  173.21 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:        eval time =  113505.89 ms /    85 runs   ( 1335.36 ms per token,     0.75 tokens per second)\n",
      "llama_print_timings:       total time =  143396.21 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```\n",
      "What is the total amount of cash sales not yet paid this month?\n",
      "How much money from cash sales remains unpaid for this month?\n",
      "Can you provide the sum of cash sales that are still outstanding for this month?\n",
      "Could you tell me the current balance of cash sales that haven't been settled for this month?\n",
      "What is the total amount of cash transactions pending payment this month?\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1329.74 ms\n",
      "llama_print_timings:      sample time =      24.10 ms /    86 runs   (    0.28 ms per token,  3568.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24354.53 ms /   140 tokens (  173.96 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =  113184.15 ms /    85 runs   ( 1331.58 ms per token,     0.75 tokens per second)\n",
      "llama_print_timings:       total time =  137924.57 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```\n",
      "Which company has the largest invoice amount?\n",
      "What is the company with the highest invoice total?\n",
      "Which business has the greatest invoiced sum?\n",
      "Which enterprise possesses the most significant invoice figure?\n",
      "Which organization boasts the highest invoice value?\n",
      "```Variations saved to train_set.xlsx (train_set) and test_set.xlsx (test_set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1329.74 ms\n",
      "llama_print_timings:      sample time =      12.59 ms /    61 runs   (    0.21 ms per token,  4846.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24442.12 ms /   140 tokens (  174.59 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:        eval time =   81871.56 ms /    60 runs   ( 1364.53 ms per token,     0.73 tokens per second)\n",
      "llama_print_timings:       total time =  106591.69 ms /   200 tokens\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from utils.common import read_yaml\n",
    "from utils.llm import LargeLanguageModel\n",
    "import random\n",
    "\n",
    "def llm_retriever(llm, question, variation_number):\n",
    "    variation_prompt = read_yaml('./prompts/generate_variation_prompt.yml')\n",
    "    system_message = variation_prompt['system_message']\n",
    "    prompt_template = variation_prompt['prompt']\n",
    "\n",
    "    prompt = prompt_template.format(query=question, number_variations=variation_number)\n",
    "\n",
    "    # Generate variations for the current question\n",
    "    generated_variations = generate_variations(llm, prompt, system_message)\n",
    "    generated_variations = generated_variations.split('```\\n')[1].split('\\n```')[0]\n",
    "    generated_variations = generated_variations.split('\\n')\n",
    "\n",
    "    # Return variation\n",
    "    return generated_variations\n",
    "\n",
    "# Run LLM\n",
    "def generate_variations(llm, prompt, system_message):\n",
    "    generated_variations = llm.llm_runnable.invoke({'system_message': system_message, 'prompt': prompt})\n",
    "    return generated_variations\n",
    "\n",
    "def read_input_file(file_path):\n",
    "    # Detect file type and read data\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.xlsx'):\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format.\")\n",
    "\n",
    "def main():\n",
    "    # Load environment variable\n",
    "    load_dotenv('./.env.development')\n",
    "\n",
    "    # Load model config\n",
    "    model_config_path = os.environ['model']\n",
    "    with open(model_config_path, 'r') as f:\n",
    "        model_config = yaml.safe_load(f)\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = LargeLanguageModel(**model_config)\n",
    "    \n",
    "    # Load questions and table descriptions\n",
    "    input_file = \"test_input.xlsx\"  # Update input file\n",
    "    questions_data = read_input_file(input_file)\n",
    "\n",
    "    variation_number = 5  # Adjust the number of variations to generate\n",
    "\n",
    "    # Accumulate variations in list\n",
    "    all_train_set = []\n",
    "    all_test_set = []\n",
    "\n",
    "    for idx, row in questions_data.iterrows():\n",
    "        question = row['Questions']\n",
    "        table_description = row['Table_Description']\n",
    "        variations = llm_retriever(llm, question, variation_number)\n",
    "\n",
    "        # Split variations into train_set and test_set\n",
    "        split_index = int(0.8 * len(variations))\n",
    "        variations_train_set = variations[:split_index]\n",
    "        variations_test_set = variations[split_index:]\n",
    "\n",
    "        # Flatten variations and accumulate 'Table_Description' and 'Positive' in respective lists\n",
    "        variations_train_set_flat = [(table_description, variation) for variation in variations_train_set]\n",
    "        variations_test_set_flat = [(table_description, variation) for variation in variations_test_set]\n",
    "\n",
    "        all_train_set.extend(variations_train_set_flat)\n",
    "        all_test_set.extend(variations_test_set_flat)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    df_train_set = pd.DataFrame(all_train_set, columns=['Table_Description', 'Positive'])\n",
    "    df_test_set = pd.DataFrame(all_test_set, columns=['Table_Description', 'Positive'])\n",
    "\n",
    "    # Save to Excel\n",
    "    output_file_train_set = \"train_set.xlsx\"  # Update output file path for train_set\n",
    "    output_file_test_set = \"test_set.xlsx\"  # Update output file path for test_set\n",
    "    \n",
    "    df_train_set.to_excel(output_file_train_set, index=False)\n",
    "    df_test_set.to_excel(output_file_test_set, index=False)\n",
    "\n",
    "    print(f\"Variations saved to {output_file_train_set} (train_set) and {output_file_test_set} (test_set)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Triplet Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to output_test2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from Excel file\n",
    "input_file = \"train_set.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a dictionary to store Positive values for each Table_Description\n",
    "positive_dict = df.groupby('Table_Description')['Positive'].apply(list).to_dict()\n",
    "\n",
    "# Create a function to get the Negative values based on the Positive values and Table_Description\n",
    "def get_negative(row):\n",
    "    table_desc = row['Table_Description']\n",
    "    positive_val = row['Positive']\n",
    "    return [val for desc, vals in positive_dict.items() if desc != table_desc for val in vals]\n",
    "\n",
    "# Apply the function to create the 'Negative' column\n",
    "df['Negative'] = df.apply(get_negative, axis=1)\n",
    "\n",
    "# Explode the 'Negative' column to create rows for each Negative value\n",
    "df_exploded = df.explode('Negative')\n",
    "\n",
    "# Reorder columns and reset index\n",
    "result_df = df_exploded[['Table_Description', 'Positive', 'Negative']].reset_index(drop=True)\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "output_file = \"output_test2.xlsx\"\n",
    "result_df.to_excel(output_file, index=False)\n",
    "print(f\"Result saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiaxin-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
